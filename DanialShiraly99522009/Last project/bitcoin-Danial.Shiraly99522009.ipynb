{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bitcoin.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QyxePuk8AiF"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVR\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.linear_model import RidgeCV\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import *\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.ensemble import VotingRegressor\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from copy import deepcopy\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QY9P4tOGpYDD"
      },
      "source": [
        "در ابتدا پس از لود کردن دیتاهای بیت کوین، تاریخ داده های را جدا کردیم و به فرمت قابل استفاده تبدیل کردیم."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "aWXIUNrnbzlX",
        "outputId": "e07e9579-b6a7-478c-ed7d-3e6a61a3467e"
      },
      "source": [
        "# data = pd.read_csv(\"drive/MyDrive/test/Bitcoin.csv\")\n",
        "data = pd.read_csv(\"/content/Bitcoin.csv\")\n",
        "\n",
        "def mdy_to_ymd(d):\n",
        "    return datetime.strptime(d, '%b %d, %Y').strftime('%Y-%m-%d')\n",
        "\n",
        "\n",
        "\n",
        "def pstr_to_flaot(P):\n",
        "    return float((\"\".join(P.split(\",\"))))\n",
        "\n",
        "data.Date = data.Date.apply(mdy_to_ymd)\n",
        "\n",
        "data.Price = data.Price.apply(pstr_to_flaot)\n",
        "data.Open = data.Open.apply(pstr_to_flaot)\n",
        "data.High = data.High.apply(pstr_to_flaot)\n",
        "data.Low = data.Low.apply(pstr_to_flaot)\n",
        "\n",
        "def vol_str_to_int(vol):\n",
        "    return vol[:-1]\n",
        "\n",
        "data[\"Vol.\"] = data[\"Vol.\"].apply(vol_str_to_int)\n",
        "\n",
        "l = []\n",
        "for i in data.index:\n",
        "    try:\n",
        "        data.iloc[i, 5] = 1000*float(data.iloc[i, 5])\n",
        "    except ValueError:\n",
        "        l.append(i)\n",
        "for i in l:\n",
        "    data.iloc[i, 5] = 0\n",
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Price</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Vol.</th>\n",
              "      <th>Change %</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2021-05-01</td>\n",
              "      <td>57807.1</td>\n",
              "      <td>57719.1</td>\n",
              "      <td>58449.4</td>\n",
              "      <td>57029.5</td>\n",
              "      <td>63410</td>\n",
              "      <td>0.15%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2021-04-30</td>\n",
              "      <td>57720.3</td>\n",
              "      <td>53562.3</td>\n",
              "      <td>57925.6</td>\n",
              "      <td>53088.7</td>\n",
              "      <td>103740</td>\n",
              "      <td>7.77%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2021-04-29</td>\n",
              "      <td>53560.8</td>\n",
              "      <td>54838.6</td>\n",
              "      <td>55173.7</td>\n",
              "      <td>52400.0</td>\n",
              "      <td>83900</td>\n",
              "      <td>-2.34%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2021-04-28</td>\n",
              "      <td>54841.4</td>\n",
              "      <td>55036.0</td>\n",
              "      <td>56419.9</td>\n",
              "      <td>53876.4</td>\n",
              "      <td>86960</td>\n",
              "      <td>-0.35%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2021-04-27</td>\n",
              "      <td>55036.5</td>\n",
              "      <td>54011.1</td>\n",
              "      <td>55427.8</td>\n",
              "      <td>53345.0</td>\n",
              "      <td>84080</td>\n",
              "      <td>1.88%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3936</th>\n",
              "      <td>2010-07-22</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>2160</td>\n",
              "      <td>0.00%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3937</th>\n",
              "      <td>2010-07-21</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>580</td>\n",
              "      <td>0.00%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3938</th>\n",
              "      <td>2010-07-20</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>260</td>\n",
              "      <td>0.00%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3939</th>\n",
              "      <td>2010-07-19</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>570</td>\n",
              "      <td>0.00%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3940</th>\n",
              "      <td>2010-07-18</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>80</td>\n",
              "      <td>0.00%</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3941 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            Date    Price     Open     High      Low    Vol. Change %\n",
              "0     2021-05-01  57807.1  57719.1  58449.4  57029.5   63410    0.15%\n",
              "1     2021-04-30  57720.3  53562.3  57925.6  53088.7  103740    7.77%\n",
              "2     2021-04-29  53560.8  54838.6  55173.7  52400.0   83900   -2.34%\n",
              "3     2021-04-28  54841.4  55036.0  56419.9  53876.4   86960   -0.35%\n",
              "4     2021-04-27  55036.5  54011.1  55427.8  53345.0   84080    1.88%\n",
              "...          ...      ...      ...      ...      ...     ...      ...\n",
              "3936  2010-07-22      0.1      0.1      0.1      0.1    2160    0.00%\n",
              "3937  2010-07-21      0.1      0.1      0.1      0.1     580    0.00%\n",
              "3938  2010-07-20      0.1      0.1      0.1      0.1     260    0.00%\n",
              "3939  2010-07-19      0.1      0.1      0.1      0.1     570    0.00%\n",
              "3940  2010-07-18      0.1      0.0      0.1      0.1      80    0.00%\n",
              "\n",
              "[3941 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "engLXgUkpzYy"
      },
      "source": [
        "سپس داده های آموزش و تست را جداسازی کرده و به صورت نامپای دراوردیم."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FoC6qfVb2yA"
      },
      "source": [
        "data_test = data.iloc[:486].to_numpy()\n",
        "data_train = data.iloc[486:].to_numpy()\n",
        "x_train, y_train = data_train[1:, 2:6], data_train[:-1, 1]\n",
        "x_test, y_test = data_test[1:, 2:6], data_test[:-1, 1]\n",
        "\n",
        "y_test_copy = deepcopy(y_test)\n",
        "y_train_copy = deepcopy(y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdlALGZbp-_I"
      },
      "source": [
        "در باکس بعدی توسط ۱۰ رگرسور، سعی کردیم که یک پیش بینی برروی  داده های تست و آموزش انجام دهیم. همانطور که مشاهده میشود، مقادیر خطای کمترین مربعات و دقت ( بصورتی که در صورت سوال خواسته شده) برای هر رگرسور نمایش داده شده است."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1cy8hsJb6mk",
        "outputId": "0267d95c-44ec-4846-8dc1-719c9ca0fc9c"
      },
      "source": [
        "LinearR = LinearRegression().fit(x_train, y_train)\n",
        "y_pred = LinearR.predict(x_test)\n",
        "print( \"1.LinearRegression accuracy = \", np.mean(np.abs(y_pred-y_test)/y_test < 0.05)*100 )\n",
        "print(\"  LinearRegression RMSE = \", mean_squared_error(y_pred, y_test))\n",
        "\n",
        "\n",
        "SVM = make_pipeline(StandardScaler(), SVR(kernel='linear',C=1000)).fit(x_train, y_train)\n",
        "y_pred = SVM.predict(x_test)\n",
        "print( \"\\n2.SVM accuracy = \", np.mean(np.abs(y_pred-y_test)/y_test < 0.05)*100 )\n",
        "print(\"  SVM RMSE = \", mean_squared_error(y_pred, y_test))\n",
        "\n",
        "\n",
        "KNR = KNeighborsRegressor(n_neighbors= 5).fit(x_train, y_train)\n",
        "y_pred = KNR.predict(x_test)\n",
        "print( \"\\n3.KNeighborsRegressor accuracy = \", np.mean(np.abs(y_pred-y_test)/y_test < 0.05)*100 )\n",
        "print(\"  KNeighborsRegressor RMSE = \", mean_squared_error(y_pred, y_test))\n",
        "\n",
        "\n",
        "RFR = RandomForestRegressor(max_depth= 5).fit(x_train, y_train)\n",
        "y_pred = RFR.predict(x_test)\n",
        "print( \"\\n4.RandomForestRegressor accuracy = \", np.mean(np.abs(y_pred-y_test)/y_test < 0.05)*100 )\n",
        "print(\"  RandomForestRegressor RMSE = \", mean_squared_error(y_pred, y_test))\n",
        "\n",
        "\n",
        "DTR = DecisionTreeRegressor().fit(x_train, y_train)\n",
        "y_pred = DTR.predict(x_test)\n",
        "print( \"\\n5.DecisionTreeRegressor accuracy = \", np.mean(np.abs(y_pred-y_test)/y_test < 0.05)*100 )\n",
        "print(\"  DecisionTreeRegressor RMSE = \", mean_squared_error(y_pred, y_test))\n",
        "\n",
        "\n",
        "SGDR = make_pipeline(StandardScaler(), SGDRegressor(max_iter=1000, tol=1e-3)).fit(x_train, y_train)\n",
        "y_pred = SGDR.predict(x_test)\n",
        "print( \"\\n6.SGDRegressor accuracy = \", np.mean(np.abs(y_pred-y_test)/y_test < 0.05)*100 )\n",
        "print(\"  SGDRegressor RMSE = \", mean_squared_error(y_pred, y_test))\n",
        "\n",
        "\n",
        "isoR = IsotonicRegression().fit(x_train[:, 0], y_train)\n",
        "y_pred = isoR.predict(x_test[:, 0])\n",
        "print( \"\\n7.IsotonicRegression accuracy = \", np.mean(np.abs(y_pred-y_test)/y_test < 0.05)*100 )\n",
        "# print(\"  IsotonicRegression RMSE = \", mean_squared_error(y_pred, y_test))\n",
        "\n",
        "\n",
        "kernel = DotProduct() + WhiteKernel()\n",
        "GPR = GaussianProcessRegressor(kernel=kernel, random_state=0).fit(x_train, y_train)\n",
        "y_pred = GPR.predict(x_test)\n",
        "print( \"\\n8.GaussianProcessRegressor accuracy = \", np.mean(np.abs(y_pred-y_test)/y_test < 0.05)*100 )\n",
        "print(\"  GaussianProcessRegressor RMSE = \", mean_squared_error(y_pred, y_test))\n",
        "\n",
        "\n",
        "MLPR = MLPRegressor(random_state=1, max_iter=500).fit(x_train, y_train)\n",
        "y_pred = MLPR.predict(x_test)\n",
        "print( \"\\n9.MLPRegressor accuracy = \", np.mean(np.abs(y_pred-y_test)/y_test < 0.05)*100 )\n",
        "print(\"  MLPRegressor RMSE = \", mean_squared_error(y_pred, y_test))\n",
        "\n",
        "\n",
        "RCVR = RidgeCV().fit(x_train, y_train)\n",
        "y_pred = RCVR.predict(x_test)\n",
        "print( \"\\n10.Ridge with Cross Validation accuracy = \", np.mean(np.abs(y_pred-y_test)/y_test < 0.05)*100)\n",
        "print(\"  Ridge with Cross Validation RMSE = \", mean_squared_error(y_pred, y_test))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.LinearRegression accuracy =  83.29896907216495\n",
            "  LinearRegression RMSE =  1463069.6228656603\n",
            "\n",
            "2.SVM accuracy =  83.91752577319588\n",
            "  SVM RMSE =  1426980.4664711824\n",
            "\n",
            "3.KNeighborsRegressor accuracy =  18.969072164948454\n",
            "  KNeighborsRegressor RMSE =  291391489.63901275\n",
            "\n",
            "4.RandomForestRegressor accuracy =  60.0\n",
            "  RandomForestRegressor RMSE =  252255787.0021648\n",
            "\n",
            "5.DecisionTreeRegressor accuracy =  38.96907216494846\n",
            "  DecisionTreeRegressor RMSE =  246340578.5049897\n",
            "\n",
            "6.SGDRegressor accuracy =  81.03092783505154\n",
            "  SGDRegressor RMSE =  1692035.3097700984\n",
            "\n",
            "7.IsotonicRegression accuracy =  52.57731958762887\n",
            "\n",
            "8.GaussianProcessRegressor accuracy =  83.50515463917526\n",
            "  GaussianProcessRegressor RMSE =  1460421.598415503\n",
            "\n",
            "9.MLPRegressor accuracy =  71.54639175257732\n",
            "  MLPRegressor RMSE =  2720706.3529329826\n",
            "\n",
            "10.Ridge with Cross Validation accuracy =  82.88659793814433\n",
            "  Ridge with Cross Validation RMSE =  1463812.0173785838\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJxxcZ81rOJ7"
      },
      "source": [
        "در این مرحله قصد داریم با کمک مدل بازگشتی شبکه عصبی برای قیمت داده های بیتکوین پیش بینی انجام دهیم. برای این کار ابتدا داده ها را استاندارد میکنیم و سپس به صورت ورودی شبکه تغییر میدهیم. در نهایت شبکه را روی مجموعه داده های ساخته شده جدید آموزش میدهیم"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eClk00NYG9ss"
      },
      "source": [
        "training_set = data.iloc[486:, 1:2].values\n",
        "test_set = data.iloc[:486, 1:2].values\n",
        "\n",
        "sc = MinMaxScaler(feature_range = (0, 1))\n",
        "training_set_scaled = sc.fit_transform(training_set)\n",
        "test_set_scaled = sc.fit_transform(test_set)\n",
        "\n",
        "X_train = []\n",
        "y_train = []\n",
        "for i in range(60, 3455):\n",
        "    X_train.append(training_set_scaled[i-60:i, 0])\n",
        "    y_train.append(training_set_scaled[i, 0])\n",
        "X_train, y_train = np.array(X_train), np.array(y_train)\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
        "\n",
        "dataset_train = data.iloc[486:, 1:2]\n",
        "dataset_test = data.iloc[:486, 1:2]\n",
        "dataset_total = pd.concat((dataset_train, dataset_test), axis = 0)\n",
        "inputs = dataset_total[len(dataset_total) - len(dataset_test) - 60:].values\n",
        "inputs = inputs.reshape(-1,1)\n",
        "inputs = sc.transform(inputs)\n",
        "X_test = []\n",
        "for i in range(60, 546):\n",
        "    X_test.append(inputs[i-60:i, 0])\n",
        "X_test = np.array(X_test)\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-ieW6-Zjc0q",
        "outputId": "109d90fe-fdcc-4da7-9c30-5d2f6a19d207"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(units = 50, return_sequences = True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(units = 50, return_sequences = True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(units = 50))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(units = 1))\n",
        "model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
        "model.fit(X_train, y_train, epochs = 20, batch_size = 64)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "54/54 [==============================] - 32s 150ms/step - loss: 0.0108\n",
            "Epoch 2/20\n",
            "54/54 [==============================] - 8s 149ms/step - loss: 0.0019\n",
            "Epoch 3/20\n",
            "54/54 [==============================] - 8s 150ms/step - loss: 0.0018\n",
            "Epoch 4/20\n",
            "54/54 [==============================] - 8s 148ms/step - loss: 0.0022\n",
            "Epoch 5/20\n",
            "54/54 [==============================] - 8s 147ms/step - loss: 0.0013\n",
            "Epoch 6/20\n",
            "54/54 [==============================] - 8s 147ms/step - loss: 0.0018\n",
            "Epoch 7/20\n",
            "54/54 [==============================] - 8s 148ms/step - loss: 0.0014\n",
            "Epoch 8/20\n",
            "54/54 [==============================] - 8s 147ms/step - loss: 0.0012\n",
            "Epoch 9/20\n",
            "54/54 [==============================] - 8s 148ms/step - loss: 9.4258e-04\n",
            "Epoch 10/20\n",
            "54/54 [==============================] - 8s 148ms/step - loss: 0.0010\n",
            "Epoch 11/20\n",
            "54/54 [==============================] - 8s 146ms/step - loss: 0.0013\n",
            "Epoch 12/20\n",
            "54/54 [==============================] - 8s 147ms/step - loss: 9.0975e-04\n",
            "Epoch 13/20\n",
            "54/54 [==============================] - 8s 147ms/step - loss: 0.0012\n",
            "Epoch 14/20\n",
            "54/54 [==============================] - 8s 148ms/step - loss: 0.0010\n",
            "Epoch 15/20\n",
            "54/54 [==============================] - 8s 148ms/step - loss: 8.9786e-04\n",
            "Epoch 16/20\n",
            "54/54 [==============================] - 8s 147ms/step - loss: 9.5101e-04\n",
            "Epoch 17/20\n",
            "54/54 [==============================] - 8s 147ms/step - loss: 7.8916e-04\n",
            "Epoch 18/20\n",
            "54/54 [==============================] - 8s 147ms/step - loss: 8.3946e-04\n",
            "Epoch 19/20\n",
            "54/54 [==============================] - 8s 147ms/step - loss: 8.0933e-04\n",
            "Epoch 20/20\n",
            "54/54 [==============================] - 8s 147ms/step - loss: 0.0010\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4241adc350>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "ptmKpNRqMiCF",
        "outputId": "c568cb86-3294-43b6-aa06-e5243d96fc20"
      },
      "source": [
        "predicted_stock_price = model.predict(X_test)\n",
        "predicted_stock_price = sc.inverse_transform(predicted_stock_price)\n",
        "\n",
        "# Visualising the results\n",
        "plt.plot(data.loc[:485, 'Date'],dataset_test.values, color='lightsteelblue')\n",
        "plt.plot(data.loc[:485, 'Date'],predicted_stock_price, color='slateblue')\n",
        "plt.title('BitCoin Price Prediction')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('BitCoin Price')\n",
        "# plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No handles with labels found to put in legend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uodjNhCguoeg"
      },
      "source": [
        "در این بخش میخواهیم از الگوریتم های انسمبل استفاده کنیم:\n",
        "\n",
        "\n",
        "*   Voting\n",
        "*   Bagging\n",
        "*   Boosting\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuvGiizUzlNe"
      },
      "source": [
        "### Voting Regressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HqgqSwHtncM",
        "outputId": "4ffd26b9-2773-4e45-ac34-5f0878043cda"
      },
      "source": [
        "y_train = y_train_copy\n",
        "y_test = y_test_copy\n",
        "stimators = [('svm', SVM), ('lr', LinearR), (\"ridge\", RCVR), (\"MLP\", MLPR), (\"sgdr\", SGDR)]\n",
        "VR = VotingRegressor([('lr', LinearR), ('svm', SVM)]).fit(x_train, y_train)\n",
        "y_pred = VR.predict(x_test)\n",
        "print( \"VotingRegressor with 2 regressor accuracy = \", np.mean(np.abs(y_pred-y_test)/y_test < 0.05)*100)\n",
        "print(\"VotingRegressor with 2 regressor RMSE = \", mean_squared_error(y_pred, y_test))\n",
        "\n",
        "VR = VotingRegressor(stimators).fit(x_train, y_train)\n",
        "y_pred = VR.predict(x_test)\n",
        "print( \"\\nVotingRegressor with 5 regressor accuracy = \", np.mean(np.abs(y_pred-y_test)/y_test < 0.05)*100)\n",
        "print(\"VotingRegressor with 5 regressor RMSE = \", mean_squared_error(y_pred, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "VotingRegressor with 2 regressor accuracy =  83.91752577319588\n",
            "VotingRegressor with 2 regressor RMSE =  1440921.2983758245\n",
            "\n",
            "VotingRegressor with 5 regressor accuracy =  82.88659793814433\n",
            "VotingRegressor with 5 regressor RMSE =  1560373.2857748242\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9YpaVTrzsQf"
      },
      "source": [
        "### Bagging Regressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDZx1IA4vzmY",
        "outputId": "242bacc0-de63-4c96-cf6c-d3b58fa6f557"
      },
      "source": [
        "VR = BaggingRegressor(SVM).fit(x_train, y_train)\n",
        "y_pred = VR.predict(x_test)\n",
        "print( \"BaggingRegressor with SVM base accuracy = \", np.mean(np.abs(y_pred-y_test)/y_test < 0.05)*100)\n",
        "print(\"BaggingRegressor with SVM base RMSE = \", mean_squared_error(y_pred, y_test))\n",
        "\n",
        "VR = BaggingRegressor(LinearR).fit(x_train, y_train)\n",
        "y_pred = VR.predict(x_test)\n",
        "print( \"\\nBaggingRegressor with LinearR base accuracy = \", np.mean(np.abs(y_pred-y_test)/y_test < 0.05)*100)\n",
        "print(\"BaggingRegressor with LinearR base RMSE = \", mean_squared_error(y_pred, y_test))\n",
        "\n",
        "VR = BaggingRegressor(RCVR).fit(x_train, y_train)\n",
        "y_pred = VR.predict(x_test)\n",
        "print( \"\\nBaggingRegressor with RidgeR base accuracy = \", np.mean(np.abs(y_pred-y_test)/y_test < 0.05)*100)\n",
        "print(\"BaggingRegressor with RidgeR base RMSE = \", mean_squared_error(y_pred, y_test))\n",
        "\n",
        "VR = BaggingRegressor(MLPR).fit(x_train, y_train)\n",
        "y_pred = VR.predict(x_test)\n",
        "print( \"\\nBaggingRegressor with MLPR base accuracy = \", np.mean(np.abs(y_pred-y_test)/y_test < 0.05)*100)\n",
        "print(\"BaggingRegressor with MLPR base RMSE = \", mean_squared_error(y_pred, y_test))\n",
        "\n",
        "VR = BaggingRegressor(SGDR).fit(x_train, y_train)\n",
        "y_pred = VR.predict(x_test)\n",
        "print( \"\\nBaggingRegressor with SGDR base accuracy = \", np.mean(np.abs(y_pred-y_test)/y_test < 0.05)*100)\n",
        "print(\"BaggingRegressor with SGDR base RMSE = \", mean_squared_error(y_pred, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BaggingRegressor with SVM base accuracy =  83.91752577319588\n",
            "BaggingRegressor with SVM base RMSE =  1429157.0507796134\n",
            "\n",
            "BaggingRegressor with LinearR base accuracy =  83.29896907216495\n",
            "BaggingRegressor with LinearR base RMSE =  1466579.7205911116\n",
            "\n",
            "BaggingRegressor with RidgeR base accuracy =  83.91752577319588\n",
            "BaggingRegressor with RidgeR base RMSE =  1481892.5382141047\n",
            "\n",
            "BaggingRegressor with MLPR base accuracy =  80.41237113402062\n",
            "BaggingRegressor with MLPR base RMSE =  1906329.258843643\n",
            "\n",
            "BaggingRegressor with SGDR base accuracy =  81.44329896907216\n",
            "BaggingRegressor with SGDR base RMSE =  1646487.401766922\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDArVbSEz5Ch"
      },
      "source": [
        "### Boosting Regressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tg8AQ1kAR6H2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dde1fbc-5bba-4c13-d37e-f44ab45a907e"
      },
      "source": [
        "VR = AdaBoostRegressor(SVM).fit(x_train, y_train)\n",
        "y_pred = VR.predict(x_test)\n",
        "print( \"AdaBoostRegressor with SVM base accuracy = \", np.mean(np.abs(y_pred-y_test)/y_test < 0.05)*100)\n",
        "print(\"AdaBoostRegressor with SVM base RMSE = \", mean_squared_error(y_pred, y_test))\n",
        "\n",
        "VR = AdaBoostRegressor(LinearR).fit(x_train, y_train)\n",
        "y_pred = VR.predict(x_test)\n",
        "print( \"\\nAdaBoostRegressor with LinearR base accuracy = \", np.mean(np.abs(y_pred-y_test)/y_test < 0.05)*100)\n",
        "print(\"AdaBoostRegressor with LinearR base RMSE = \", mean_squared_error(y_pred, y_test))\n",
        "\n",
        "VR = AdaBoostRegressor(RCVR).fit(x_train, y_train)\n",
        "y_pred = VR.predict(x_test)\n",
        "print( \"\\nAdaBoostRegressor with RidgeR base accuracy = \", np.mean(np.abs(y_pred-y_test)/y_test < 0.05)*100)\n",
        "print(\"AdaBoostRegressor with RidgeR base RMSE = \", mean_squared_error(y_pred, y_test))\n",
        "\n",
        "VR = AdaBoostRegressor(MLPR).fit(x_train, y_train)\n",
        "y_pred = VR.predict(x_test)\n",
        "print( \"\\nAdaBoostRegressor with MLPR base accuracy = \", np.mean(np.abs(y_pred-y_test)/y_test < 0.05)*100)\n",
        "print(\"AdaBoostRegressor with MLPR base RMSE = \", mean_squared_error(y_pred, y_test))\n",
        "\n",
        "VR = AdaBoostRegressor(SGDR).fit(x_train, y_train)\n",
        "y_pred = VR.predict(x_test)\n",
        "print( \"\\nAdaBoostRegressor with SGDR base accuracy = \", np.mean(np.abs(y_pred-y_test)/y_test < 0.05)*100)\n",
        "print(\"AdaBoostRegressor with SGDR base RMSE = \", mean_squared_error(y_pred, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AdaBoostRegressor with SVM base accuracy =  83.50515463917526\n",
            "AdaBoostRegressor with SVM base RMSE =  1501224.4250258896\n",
            "\n",
            "AdaBoostRegressor with LinearR base accuracy =  79.79381443298969\n",
            "AdaBoostRegressor with LinearR base RMSE =  1831821.114212425\n",
            "\n",
            "AdaBoostRegressor with RidgeR base accuracy =  81.64948453608247\n",
            "AdaBoostRegressor with RidgeR base RMSE =  1610028.1702815546\n",
            "\n",
            "AdaBoostRegressor with MLPR base accuracy =  74.63917525773196\n",
            "AdaBoostRegressor with MLPR base RMSE =  2182791.5036949837\n",
            "\n",
            "AdaBoostRegressor with SGDR base accuracy =  82.68041237113401\n",
            "AdaBoostRegressor with SGDR base RMSE =  1452505.831113617\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwIPF_tjvPGL"
      },
      "source": [
        "همانگونه که دیده میشود عملکرد  بگینگ در مقایسه با بوستینگ بطور جداگانه کمی بهتر بوده است. همینطور استفاده از ووتینگ نیز باعث افزایش دقت شده است."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZ2vZpqD7JtB"
      },
      "source": [
        "### Random Forest Method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clzv9YtUv9K1"
      },
      "source": [
        "در اینجا متود رندوم فارست برای پیش بینی قیمت استفاده کردیم و همانطور که دیده میشود، افزودن ماکسیمم عمق ۵ به درختان، باعث بهبود عملکرد خواهد شد"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeOz5ws9rZtI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "039df5dd-b2dc-4bdd-a074-293715a71856"
      },
      "source": [
        "RFR = RandomForestRegressor().fit(x_train, y_train)\n",
        "y_pred = RFR.predict(x_test)\n",
        "print( \"RandomForestRegressor accuracy = \", np.mean(np.abs(y_pred-y_test)/y_test < 0.05)*100 )\n",
        "print(\"RandomForestRegressor RMSE = \", mean_squared_error(y_pred, y_test))\n",
        "\n",
        "RFR = RandomForestRegressor(max_depth=5).fit(x_train, y_train)\n",
        "y_pred = RFR.predict(x_test)\n",
        "print( \"\\nRandomForestRegressor with maximum depth 5 accuracy = \", np.mean(np.abs(y_pred-y_test)/y_test < 0.05)*100 )\n",
        "print(\"RandomForestRegressor with maximum depth 5 RMSE = \", mean_squared_error(y_pred, y_test))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RandomForestRegressor accuracy =  55.25773195876289\n",
            "RandomForestRegressor RMSE =  250118725.77573574\n",
            "\n",
            "RandomForestRegressor with maximum depth5 accuracy =  60.20618556701031\n",
            "RandomForestRegressor with maximum depth 5 RMSE =  250047783.03336406\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ub8ndDTtwWiE"
      },
      "source": [
        "در این قسمت سعی کردیم ابتدا یک ستون شامل داده هایی درست کنیم که نشان دهنده بالاتر رفتن یا پایین آمدن قیمت نسبت به روز قبلی باشند، در این صورت، میتوان مساله را به یک مساله کلاسبندی تبدیل کرد که وظیفه مدل شبکه عصبی، پیش بینی بالارفتن یا پایین آمدن قیمت در روز بعدی باشد(کلاسبندی)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmx-ECPH1Vf1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29905609-c155-47ca-b8ac-9ccb8fca9fd1"
      },
      "source": [
        "data[\"change_sign\"] = np.where( (data.Price - data.Open) >= 0, 1, 0)\n",
        "data.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3941 entries, 0 to 3940\n",
            "Data columns (total 8 columns):\n",
            " #   Column       Non-Null Count  Dtype  \n",
            "---  ------       --------------  -----  \n",
            " 0   Date         3941 non-null   object \n",
            " 1   Price        3941 non-null   float64\n",
            " 2   Open         3941 non-null   float64\n",
            " 3   High         3941 non-null   float64\n",
            " 4   Low          3941 non-null   float64\n",
            " 5   Vol.         3941 non-null   object \n",
            " 6   Change %     3941 non-null   object \n",
            " 7   change_sign  3941 non-null   int64  \n",
            "dtypes: float64(4), int64(1), object(3)\n",
            "memory usage: 246.4+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_q4-D606mhg3"
      },
      "source": [
        "training_set = data.iloc[486:, 1:2].values\n",
        "test_set = data.iloc[:486, 1:2].values\n",
        "y_test = data.iloc[60:486, -1].values\n",
        "y_train = data.iloc[546:, -1].values\n",
        "\n",
        "\n",
        "sc = MinMaxScaler(feature_range = (0, 1))\n",
        "training_set_scaled = sc.fit_transform(training_set)\n",
        "test_set_scaled = sc.fit_transform(test_set)\n",
        "\n",
        "X_train = []\n",
        "# y_train = []\n",
        "for i in range(60, 3455):\n",
        "    X_train.append(training_set_scaled[i-60:i, 0])\n",
        "    # y_train.append(training_set_scaled[i, 7])\n",
        "X_train, y_train = np.array(X_train), np.array(y_train)\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
        "\n",
        "dataset_train = data.iloc[486:, 1:2]\n",
        "dataset_test = data.iloc[:486, 1:2]\n",
        "dataset_total = pd.concat((dataset_train, dataset_test), axis = 0)\n",
        "inputs = dataset_total[len(dataset_total) - len(dataset_test) - 60:].values\n",
        "inputs = inputs.reshape(-1,1)\n",
        "inputs = sc.transform(inputs)\n",
        "X_test = []\n",
        "# y_test = []\n",
        "for i in range(60, 546):\n",
        "    X_test.append(inputs[i-60:i, 0])\n",
        "    # y_test.append(training_set_scaled[i, 7])\n",
        "\n",
        "X_test = np.array(X_test)\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CORJ8M51kLTZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a0dbf92-e9e5-4bd8-c201-172099b2c270"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(units = 50, return_sequences = True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(units = 50, return_sequences = True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(units = 50))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(units = 1,  activation='sigmoid'))\n",
        "model.compile(optimizer = 'adam', loss = 'mean_squared_error', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, epochs = 20, batch_size = 64)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "54/54 [==============================] - 16s 149ms/step - loss: 0.2446 - accuracy: 0.5866\n",
            "Epoch 2/20\n",
            "54/54 [==============================] - 8s 148ms/step - loss: 0.2421 - accuracy: 0.5812\n",
            "Epoch 3/20\n",
            "54/54 [==============================] - 8s 146ms/step - loss: 0.2418 - accuracy: 0.5868\n",
            "Epoch 4/20\n",
            "54/54 [==============================] - 8s 148ms/step - loss: 0.2406 - accuracy: 0.5819\n",
            "Epoch 5/20\n",
            "54/54 [==============================] - 8s 148ms/step - loss: 0.2391 - accuracy: 0.5985\n",
            "Epoch 6/20\n",
            "54/54 [==============================] - 8s 147ms/step - loss: 0.2401 - accuracy: 0.5851\n",
            "Epoch 7/20\n",
            "54/54 [==============================] - 8s 147ms/step - loss: 0.2395 - accuracy: 0.5743\n",
            "Epoch 8/20\n",
            "54/54 [==============================] - 8s 147ms/step - loss: 0.2429 - accuracy: 0.5809\n",
            "Epoch 9/20\n",
            "54/54 [==============================] - 8s 147ms/step - loss: 0.2390 - accuracy: 0.5977\n",
            "Epoch 10/20\n",
            "54/54 [==============================] - 8s 149ms/step - loss: 0.2377 - accuracy: 0.6112\n",
            "Epoch 11/20\n",
            "54/54 [==============================] - 8s 147ms/step - loss: 0.2392 - accuracy: 0.5936\n",
            "Epoch 12/20\n",
            "54/54 [==============================] - 8s 147ms/step - loss: 0.2389 - accuracy: 0.5913\n",
            "Epoch 13/20\n",
            "54/54 [==============================] - 8s 149ms/step - loss: 0.2391 - accuracy: 0.5922\n",
            "Epoch 14/20\n",
            "54/54 [==============================] - 8s 148ms/step - loss: 0.2419 - accuracy: 0.5918\n",
            "Epoch 15/20\n",
            "54/54 [==============================] - 8s 148ms/step - loss: 0.2380 - accuracy: 0.5952\n",
            "Epoch 16/20\n",
            "54/54 [==============================] - 8s 147ms/step - loss: 0.2375 - accuracy: 0.5918\n",
            "Epoch 17/20\n",
            "54/54 [==============================] - 8s 147ms/step - loss: 0.2416 - accuracy: 0.5876\n",
            "Epoch 18/20\n",
            "54/54 [==============================] - 8s 147ms/step - loss: 0.2369 - accuracy: 0.5909\n",
            "Epoch 19/20\n",
            "54/54 [==============================] - 8s 147ms/step - loss: 0.2362 - accuracy: 0.5971\n",
            "Epoch 20/20\n",
            "54/54 [==============================] - 8s 148ms/step - loss: 0.2387 - accuracy: 0.6087\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f423c3eddd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rD9a7j6rvEVv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f90d18ab-bdad-43f8-8d2e-5fef51049e24"
      },
      "source": [
        "pip install ta"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ta in /usr/local/lib/python3.7/dist-packages (0.7.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from ta) (1.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from ta) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->ta) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->ta) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->ta) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rV1masInqOq"
      },
      "source": [
        "<div style=\"text-align:right\"> .در این قسمت دو اندیکاتور را به عنوان ویژگی به مجموعه داده ها اضافه کردیم.همان گونه ک مشاهده می کنید اضافه کردن دو اندیکاتور هم می توان اثر افزایشی هرچند اندکی بر دقت داشته باشد</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "8fIDaZ48W15M",
        "outputId": "19910178-da5c-46bf-b6a9-7c422298f29a"
      },
      "source": [
        "import ta\n",
        "\n",
        "def add_Indicators_data(data): # data is a pandas.DataFrame\n",
        "    data[\"CCI\"] = ta.trend.CCIIndicator(high = data[\"High\"], low = data[\"Low\"], close = data[\"Price\"], window = 20, constant = 0.015, fillna = True).cci() \n",
        "    data[\"SMA\"] = ta.trend.sma_indicator(data[\"Price\"], window=53, fillna=True)\n",
        "    return data\n",
        "\n",
        "indicators = add_Indicators_data(data).drop([\"Change %\"], axis=1)\n",
        "indicators"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Price</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Vol.</th>\n",
              "      <th>CCI</th>\n",
              "      <th>SMA</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2021-05-01</td>\n",
              "      <td>57807.1</td>\n",
              "      <td>57719.1</td>\n",
              "      <td>58449.4</td>\n",
              "      <td>57029.5</td>\n",
              "      <td>63410</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>57807.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2021-04-30</td>\n",
              "      <td>57720.3</td>\n",
              "      <td>53562.3</td>\n",
              "      <td>57925.6</td>\n",
              "      <td>53088.7</td>\n",
              "      <td>103740</td>\n",
              "      <td>-66.666667</td>\n",
              "      <td>57763.700000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2021-04-29</td>\n",
              "      <td>53560.8</td>\n",
              "      <td>54838.6</td>\n",
              "      <td>55173.7</td>\n",
              "      <td>52400.0</td>\n",
              "      <td>83900</td>\n",
              "      <td>-100.000000</td>\n",
              "      <td>56362.733333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2021-04-28</td>\n",
              "      <td>54841.4</td>\n",
              "      <td>55036.0</td>\n",
              "      <td>56419.9</td>\n",
              "      <td>53876.4</td>\n",
              "      <td>86960</td>\n",
              "      <td>-32.773700</td>\n",
              "      <td>55982.400000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2021-04-27</td>\n",
              "      <td>55036.5</td>\n",
              "      <td>54011.1</td>\n",
              "      <td>55427.8</td>\n",
              "      <td>53345.0</td>\n",
              "      <td>84080</td>\n",
              "      <td>-47.407194</td>\n",
              "      <td>55793.220000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3936</th>\n",
              "      <td>2010-07-22</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>2160</td>\n",
              "      <td>35.087719</td>\n",
              "      <td>0.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3937</th>\n",
              "      <td>2010-07-21</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>580</td>\n",
              "      <td>35.087719</td>\n",
              "      <td>0.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3938</th>\n",
              "      <td>2010-07-20</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>260</td>\n",
              "      <td>35.087719</td>\n",
              "      <td>0.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3939</th>\n",
              "      <td>2010-07-19</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>570</td>\n",
              "      <td>35.087719</td>\n",
              "      <td>0.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3940</th>\n",
              "      <td>2010-07-18</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>80</td>\n",
              "      <td>35.087719</td>\n",
              "      <td>0.100000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3941 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            Date    Price     Open  ...    Vol.         CCI           SMA\n",
              "0     2021-05-01  57807.1  57719.1  ...   63410    0.000000  57807.100000\n",
              "1     2021-04-30  57720.3  53562.3  ...  103740  -66.666667  57763.700000\n",
              "2     2021-04-29  53560.8  54838.6  ...   83900 -100.000000  56362.733333\n",
              "3     2021-04-28  54841.4  55036.0  ...   86960  -32.773700  55982.400000\n",
              "4     2021-04-27  55036.5  54011.1  ...   84080  -47.407194  55793.220000\n",
              "...          ...      ...      ...  ...     ...         ...           ...\n",
              "3936  2010-07-22      0.1      0.1  ...    2160   35.087719      0.100000\n",
              "3937  2010-07-21      0.1      0.1  ...     580   35.087719      0.100000\n",
              "3938  2010-07-20      0.1      0.1  ...     260   35.087719      0.100000\n",
              "3939  2010-07-19      0.1      0.1  ...     570   35.087719      0.100000\n",
              "3940  2010-07-18      0.1      0.0  ...      80   35.087719      0.100000\n",
              "\n",
              "[3941 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAI0ISNvYhLv"
      },
      "source": [
        "data_test = indicators.iloc[:486].to_numpy()\n",
        "data_train = indicators.iloc[486:].to_numpy()\n",
        "x_train, y_train = data_train[1:, 2:], data_train[:-1, 1]\n",
        "x_test, y_test = data_test[1:, 2:], data_test[:-1, 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVt8EUj3ZEtO",
        "outputId": "d964c6cc-a817-467e-edd7-fdc3b449c219"
      },
      "source": [
        "LinearR = LinearRegression().fit(x_train, y_train)\n",
        "y_pred = LinearR.predict(x_test)\n",
        "print( \"1.LinearRegression accuracy = \", np.mean(np.abs(y_pred-y_test)/y_test < 0.05)*100 )\n",
        "print(\"  LinearRegression RMSE = \", mean_squared_error(y_pred, y_test))\n",
        "\n",
        "\n",
        "SVM = make_pipeline(StandardScaler(), SVR(kernel='linear',C=1000)).fit(x_train, y_train)\n",
        "y_pred = SVM.predict(x_test)\n",
        "print( \"\\n2.SVM accuracy = \", np.mean(np.abs(y_pred-y_test)/y_test < 0.05)*100 )\n",
        "print(\"  SVM RMSE = \", mean_squared_error(y_pred, y_test))\n",
        "\n",
        "\n",
        "KNR = KNeighborsRegressor(n_neighbors= 5).fit(x_train, y_train)\n",
        "y_pred = KNR.predict(x_test)\n",
        "print( \"\\n3.KNeighborsRegressor accuracy = \", np.mean(np.abs(y_pred-y_test)/y_test < 0.05)*100 )\n",
        "print(\"  KNeighborsRegressor RMSE = \", mean_squared_error(y_pred, y_test))\n",
        "\n",
        "\n",
        "RFR = RandomForestRegressor(max_depth= 5).fit(x_train, y_train)\n",
        "y_pred = RFR.predict(x_test)\n",
        "print( \"\\n4.RandomForestRegressor accuracy = \", np.mean(np.abs(y_pred-y_test)/y_test < 0.05)*100 )\n",
        "print(\"  RandomForestRegressor RMSE = \", mean_squared_error(y_pred, y_test))\n",
        "\n",
        "\n",
        "DTR = DecisionTreeRegressor().fit(x_train, y_train)\n",
        "y_pred = DTR.predict(x_test)\n",
        "print( \"\\n5.DecisionTreeRegressor accuracy = \", np.mean(np.abs(y_pred-y_test)/y_test < 0.05)*100 )\n",
        "print(\"  DecisionTreeRegressor RMSE = \", mean_squared_error(y_pred, y_test))\n",
        "\n",
        "\n",
        "SGDR = make_pipeline(StandardScaler(), SGDRegressor(max_iter=1000, tol=1e-3)).fit(x_train, y_train)\n",
        "y_pred = SGDR.predict(x_test)\n",
        "print( \"\\n6.SGDRegressor accuracy = \", np.mean(np.abs(y_pred-y_test)/y_test < 0.05)*100 )\n",
        "print(\"  SGDRegressor RMSE = \", mean_squared_error(y_pred, y_test))\n",
        "\n",
        "\n",
        "isoR = IsotonicRegression().fit(x_train[:, 0], y_train)\n",
        "y_pred = isoR.predict(x_test[:, 0])\n",
        "print( \"\\n7.IsotonicRegression accuracy = \", np.mean(np.abs(y_pred-y_test)/y_test < 0.05)*100 )\n",
        "# print(\"  IsotonicRegression RMSE = \", mean_squared_error(y_pred, y_test))\n",
        "\n",
        "\n",
        "kernel = DotProduct() + WhiteKernel()\n",
        "GPR = GaussianProcessRegressor(kernel=kernel, random_state=0).fit(x_train, y_train)\n",
        "y_pred = GPR.predict(x_test)\n",
        "print( \"\\n8.GaussianProcessRegressor accuracy = \", np.mean(np.abs(y_pred-y_test)/y_test < 0.05)*100 )\n",
        "print(\"  GaussianProcessRegressor RMSE = \", mean_squared_error(y_pred, y_test))\n",
        "\n",
        "\n",
        "MLPR = MLPRegressor(random_state=1, max_iter=500).fit(x_train, y_train)\n",
        "y_pred = MLPR.predict(x_test)\n",
        "print( \"\\n9.MLPRegressor accuracy = \", np.mean(np.abs(y_pred-y_test)/y_test < 0.05)*100 )\n",
        "print(\"  MLPRegressor RMSE = \", mean_squared_error(y_pred, y_test))\n",
        "\n",
        "\n",
        "RCVR = RidgeCV().fit(x_train, y_train)\n",
        "y_pred = RCVR.predict(x_test)\n",
        "print( \"\\n10.Ridge with Cross Validation accuracy = \", np.mean(np.abs(y_pred-y_test)/y_test < 0.05)*100)\n",
        "print(\"  Ridge with Cross Validation RMSE = \", mean_squared_error(y_pred, y_test))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.LinearRegression accuracy =  83.29896907216495\n",
            "  LinearRegression RMSE =  1328557.80847682\n",
            "\n",
            "2.SVM accuracy =  83.29896907216495\n",
            "  SVM RMSE =  1345198.342708777\n",
            "\n",
            "3.KNeighborsRegressor accuracy =  21.030927835051546\n",
            "  KNeighborsRegressor RMSE =  285444396.8386928\n",
            "\n",
            "4.RandomForestRegressor accuracy =  58.35051546391753\n",
            "  RandomForestRegressor RMSE =  254660594.2809811\n",
            "\n",
            "5.DecisionTreeRegressor accuracy =  32.78350515463917\n",
            "  DecisionTreeRegressor RMSE =  283305788.13247424\n",
            "\n",
            "6.SGDRegressor accuracy =  80.82474226804123\n",
            "  SGDRegressor RMSE =  1490798.8585655869\n",
            "\n",
            "7.IsotonicRegression accuracy =  52.57731958762887\n",
            "\n",
            "8.GaussianProcessRegressor accuracy =  83.50515463917526\n",
            "  GaussianProcessRegressor RMSE =  1328674.038116377\n",
            "\n",
            "9.MLPRegressor accuracy =  76.08247422680412\n",
            "  MLPRegressor RMSE =  1767012.9395031224\n",
            "\n",
            "10.Ridge with Cross Validation accuracy =  83.71134020618557\n",
            "  Ridge with Cross Validation RMSE =  1329805.0675473476\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pu5HrJJcaKrz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}